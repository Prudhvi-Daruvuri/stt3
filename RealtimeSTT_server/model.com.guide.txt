[Guide](https://modal.com/docs/guide)
[Examples](https://modal.com/docs/examples)
[Reference](https://modal.com/docs/reference)
[Playground](https://modal.com/playground)
[Log In](https://modal.com/login?next=%2Fapps)
[Sign Up](https://modal.com/signup?next=%2Fapps)
[Featured](https://modal.com/docs/examples)
[Getting started](https://modal.com/docs/examples/hello_world)
[Hello, world](https://modal.com/docs/examples/hello_world)
[Simple web scraper](https://modal.com/docs/examples/web-scraper)
[Serving web endpoints](https://modal.com/docs/examples/basic_web)
[Large language models (LLMs)](https://modal.com/docs/examples/vllm_inference)
[Deploy an OpenAI-compatible LLM service with vLLM](https://modal.com/docs/examples/vllm_inference)
[Run DeepSeek-R1 and Phi-4 with llama.cpp](https://modal.com/docs/examples/llama_cpp)
[Low-latency, serverless TensorRT-LLM](https://modal.com/docs/examples/trtllm_latency)
[Run Vision-Language Models with SGLang](https://modal.com/docs/examples/sgl_vlm)
[Run a multimodal RAG chatbot to answer questions about PDFs](https://modal.com/docs/examples/chat_with_pdf_vision)
[Deploy and benchmark the Tokasaurus high-throughput LLM server](https://modal.com/docs/examples/tokasaurus_throughput)
[Fine-tune an LLM to replace your CEO](https://modal.com/docs/examples/llm-finetuning)
[Images, video, & 3D](https://modal.com/docs/examples/image_to_image)
[Edit images with Flux Kontext](https://modal.com/docs/examples/image_to_image)
[Fine-tune Wan2.1 video models on your face](https://modal.com/docs/examples/music-video-gen)
[Run Flux fast with torch.compile](https://modal.com/docs/examples/flux)
[Fine-tune Flux with LoRA](https://modal.com/docs/examples/diffusers_lora_finetune)
[Animate images with LTX-Video](https://modal.com/docs/examples/image_to_video)
[Generate video clips with LTX-Video](https://modal.com/docs/examples/ltx)
[Run Stable Diffusion with a CLI, API, and web UI](https://modal.com/docs/examples/stable_diffusion_cli)
[Audio](https://modal.com/docs/examples/chatterbox_tts)
[Generate speech with Chatterbox](https://modal.com/docs/examples/chatterbox_tts)
[Deploy a Moshi voice chatbot](https://modal.com/docs/examples/llm-voice-chat)
[Stream transcripts at the speed of speech using Kyutai STT](https://modal.com/docs/examples/streaming_kyutai_stt)
[Run high throughput batched transcription with Whisper](https://modal.com/docs/examples/batched_whisper)
[Create music with MusicGen](https://modal.com/docs/examples/musicgen)
[Real-time communication (WebRTC)](https://modal.com/docs/examples/webrtc_yolo)
[Serverless WebRTC](https://modal.com/docs/examples/webrtc_yolo)
[WebRTC quickstart with FastRTC](https://modal.com/docs/examples/fastrtc_flip_webcam)
[Computational biology](https://modal.com/docs/examples/chai1)
[Fold proteins with Chai-1](https://modal.com/docs/examples/chai1)
[Build a protein-folding dashboard](https://modal.com/docs/examples/esm3)
[Fold proteins with Boltz-2](https://modal.com/docs/examples/boltz_predict)
[Modal Sandboxes](https://modal.com/docs/examples/agent)
[Run a LangGraph agent's code in a secure GPU sandbox](https://modal.com/docs/examples/agent)
[Control a sandboxed computer with an LLM](https://modal.com/docs/examples/anthropic_computer_use)
[Build a stateful, sandboxed code interpreter](https://modal.com/docs/examples/simple_code_interpreter)
[Run Node.js, Ruby, and more in a Sandbox](https://modal.com/docs/examples/safe_code_execution)
[Run a sandboxed Jupyter notebook](https://modal.com/docs/examples/jupyter_sandbox)
[Embeddings](https://modal.com/docs/examples/amazon_embeddings)
[Embed millions of documents with TEI](https://modal.com/docs/examples/amazon_embeddings)
[Turn satellite images into vectors and store them in MongoDB](https://modal.com/docs/examples/mongodb-search)
[Parallel processing and job scheduling](https://modal.com/docs/examples/whisper-transcriber)
[Transcribe podcasts with Whisper](https://modal.com/docs/examples/whisper-transcriber)
[Deploy a Hacker News Slackbot](https://modal.com/docs/examples/hackernews_alerts)
[Run a Document OCR job queue](https://modal.com/docs/examples/doc_ocr_jobs)
[Serve a Document OCR web app](https://modal.com/docs/examples/doc_ocr_webapp)
[Training models from scratch](https://modal.com/docs/examples/hp_sweep_gpt)
[Train an SLM with early-stopping grid search over hyperparameters](https://modal.com/docs/examples/hp_sweep_gpt)
[Run long, resumable training jobs](https://modal.com/docs/examples/long-training)

[Hosting popular libraries](https://modal.com/docs/examples/finetune_yolo)
[YOLO: Fine-tune and serve computer vision models](https://modal.com/docs/examples/finetune_yolo)
[Blender: Build a 3D render farm](https://modal.com/docs/examples/blender_video)
[Streamlit: Run and deploy Streamlit apps](https://modal.com/docs/examples/serve_streamlit)
[ComfyUI: Run Flux on ComfyUI as an API](https://modal.com/docs/examples/comfyapp)
[SQLite: Publish explorable data with Datasette](https://modal.com/docs/examples/cron_datasette)
[Algolia: Build docsearch with a crawler](https://modal.com/docs/examples/algolia_indexer)
[Connecting to other APIs](https://modal.com/docs/examples/discord_bot)
[Discord: Deploy and run a Discord Bot](https://modal.com/docs/examples/discord_bot)
[Google Sheets: Sync databases and APIs to a Google Sheet](https://modal.com/docs/examples/db_to_sheet)
[OpenAI: Run a RAG Q&A chatbot](https://modal.com/docs/examples/potus_speech_qanda)
[Tailscale: Add Modal Apps to your VPN](https://modal.com/docs/examples/modal_tailscale)
[Prometheus: Publish custom metrics with Pushgateway](https://modal.com/docs/examples/pushgateway)
[Managing data](https://modal.com/docs/examples/s3_bucket_mount)
[Mount S3 buckets in Modal apps](https://modal.com/docs/examples/s3_bucket_mount)
[Build your own data warehouse with DuckDB, DBT, and Modal](https://modal.com/docs/examples/dbt_duckdb)
[Create a LoRA Playground with Modal, Gradio, and S3](https://modal.com/docs/examples/cloud_bucket_mount_loras)
[Miscellaneous](https://modal.com/docs/examples/miscellaneous)
[View on GitHub](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/speech-to-text/streaming_parakeet.py)

# Streaming audio transcription using Parakeet

This examples demonstrates the use of Parakeet ASR models for streaming speech-to-text on Modal.
Parakeet is the name of a family of ASR models built using [NVIDIAâ€™s NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html). Weâ€™ll show you how to use Parakeet for streaming audio transcription on Modal GPUs, with simple Python and browser clients.
This example uses the nvidia/parakeet-tdt-0.6b-v2 model which, as of June 2025, sits at the top of Hugging Faceâ€™s [Open ASR leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).

```
nvidia/parakeet-tdt-0.6b-v2
```

To try out transcription from your terminal, provide a URL for a .wav file to modal run:

```
.wav
```


```
modal run
```


```
modal run 06_gpu_and_ml/parakeet/parakeet.py --audio-url="https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/mono_44100/156550__acclivity__a-dream-within-a-dream.wav"
```


```
modal run 06_gpu_and_ml/parakeet/parakeet.py --audio-url="https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/mono_44100/156550__acclivity__a-dream-within-a-dream.wav"
```

You should see output like the following:

```
ðŸŽ¤ Starting Transcription A Dream Within A Dream Edgar Allan Poe take this kiss upon the brow, And in parting from you now, Thus much let me avow You are not wrong who deem That my days have been a dream. ...
```


```
ðŸŽ¤ Starting Transcription A Dream Within A Dream Edgar Allan Poe take this kiss upon the brow, And in parting from you now, Thus much let me avow You are not wrong who deem That my days have been a dream. ...
```

Running a web service you can hit from any browser isnâ€™t any harder â€” Modal handles the deployment of both the frontend and backend in a single App! Just run

```
modal serve 06_gpu_and_ml/parakeet/parakeet.py
```


```
modal serve 06_gpu_and_ml/parakeet/parakeet.py
```

and go to the link printed in your terminal.
The full frontend code can be found [here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/parakeet-frontend/).

## Setup
```
import asyncio import os import sys from pathlib import Path import modal app = modal.App("example-streaming-parakeet")
```


```
import asyncio import os import sys from pathlib import Path import modal app = modal.App("example-streaming-parakeet")
```

## Volume for caching model weights
We use a [Modal Volume](https://modal.com/docs/guide/volumes) to cache the model weights. This allows us to avoid downloading the model weights every time we start a new instance.
For more on storing models on Modal, see [this guide](https://modal.com/docs/guide/model-weights).

```
model_cache = modal.Volume.from_name("parakeet-model-cache", create_if_missing=True)
```


```
model_cache = modal.Volume.from_name("parakeet-model-cache", create_if_missing=True)
```

## Configuring dependencies
The model runs remotely inside a container on Modal. We can define the environment and install our Python dependencies in that containerâ€™s [Image](https://modal.com/docs/guide/images).

```
Image
```

For finicky setups like NeMOâ€™s, we recommend using the official NVIDIA CUDA Docker images from Docker Hub. Youâ€™ll need to install Python and pip with the add_python option because the image doesnâ€™t have these by default.

```
add_python
```

Additionally, we install ffmpeg for handling audio data and fastapi to create a web server for our WebSocket.

```
ffmpeg
```


```
fastapi
```


```
image = ( modal.Image.from_registry( "nvidia/cuda:12.8.0-cudnn-devel-ubuntu22.04", add_python="3.12" ) .env( { "HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HOME": "/cache", # cache directory for Hugging Face models "DEBIAN_FRONTEND": "noninteractive", "CXX": "g++", "CC": "g++", } ) .apt_install("ffmpeg") .pip_install( "hf_transfer==0.1.9", "huggingface_hub[hf-xet]==0.31.2", "nemo_toolkit[asr]==2.3.0", "cuda-python==12.8.0", "fastapi==0.115.12", "numpy<2", "pydub==0.25.1", ) .entrypoint([]) # silence chatty logs by container on start .add_local_dir( # changes fastest, so make this the last layer Path(__file__).parent / "streaming-parakeet-frontend", remote_path="/frontend", ) )
```


```
image = ( modal.Image.from_registry( "nvidia/cuda:12.8.0-cudnn-devel-ubuntu22.04", add_python="3.12" ) .env( { "HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HOME": "/cache", # cache directory for Hugging Face models "DEBIAN_FRONTEND": "noninteractive", "CXX": "g++", "CC": "g++", } ) .apt_install("ffmpeg") .pip_install( "hf_transfer==0.1.9", "huggingface_hub[hf-xet]==0.31.2", "nemo_toolkit[asr]==2.3.0", "cuda-python==12.8.0", "fastapi==0.115.12", "numpy<2", "pydub==0.25.1", ) .entrypoint([]) # silence chatty logs by container on start .add_local_dir( # changes fastest, so make this the last layer Path(__file__).parent / "streaming-parakeet-frontend", remote_path="/frontend", ) )
```

## Implementing streaming audio transcription on Modal
Now weâ€™re ready to implement transcription. We wrap inference in a [modal.Cls](https://modal.com/docs/guide/lifecycle-functions) that ensures models are loaded and then moved to the GPU once when a new container starts.

```
modal.Cls
```

A couples of notes about this code:
- The transcribe method takes bytes of audio data and returns the transcribed text.
- The web method creates a FastAPI app using modal.asgi_app that serves a [WebSocket](https://modal.com/docs/guide/webhooks) endpoint for streaming audio transcription and a browser frontend for transcribing audio from your microphone.
- The run_with_queue method takes a [modal.Queue](https://modal.com/docs/reference/modal.Queue) and passes audio data and transcriptions between our local machine and the GPU container.

```
transcribe
```


```
web
```

[modal.asgi_app](https://modal.com/docs/reference/modal.asgi_app)

```
modal.asgi_app
```

[WebSocket](https://modal.com/docs/guide/webhooks)

```
run_with_queue
```

[modal.Queue](https://modal.com/docs/reference/modal.Queue)

```
modal.Queue
```

Parakeet tries really hard to transcribe everything to English! Hence it tends to output utterances like â€œYeahâ€ or â€œMm-hmmâ€ when it runs on silent audio. We pre-process the incoming audio in the server using pydubâ€™s silence detection, ensuring that we donâ€™t pass silence into our model.

```
pydub
```

## Implementing streaming audio transcription on Modal

```
END_OF_STREAM = ( b"END_OF_STREAM_8f13d09" # byte sequence indicating a stream is finished ) @app.cls(volumes={"/cache": model_cache}, gpu="a10g", image=image) @modal.concurrent(max_inputs=14, target_inputs=10) class Parakeet: @modal.enter() def load(self): import logging import nemo.collections.asr as nemo_asr # silence chatty logs from nemo logging.getLogger("nemo_logger").setLevel(logging.CRITICAL) self.model = nemo_asr.models.ASRModel.from_pretrained( model_name="nvidia/parakeet-tdt-0.6b-v2" ) def transcribe(self, audio_bytes: bytes) -> str: import numpy as np audio_data = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) with NoStdStreams(): # hide output, see https://github.com/NVIDIA/NeMo/discussions/3281#discussioncomment-2251217 output = self.model.transcribe([audio_data]) return output[0].text @modal.asgi_app() def web(self): from fastapi import FastAPI, Response, WebSocket from fastapi.responses import HTMLResponse from fastapi.staticfiles import StaticFiles web_app = FastAPI() web_app.mount("/static", StaticFiles(directory="/frontend")) @web_app.get("/status") async def status(): return Response(status_code=200) # serve frontend @web_app.get("/") async def index(): return HTMLResponse(content=open("/frontend/index.html").read()) @web_app.websocket("/ws") async def run_with_websocket(ws: WebSocket): from fastapi import WebSocketDisconnect from pydub import AudioSegment await ws.accept() # initialize an empty audio segment audio_segment = AudioSegment.empty() try: while True: # receive a chunk of audio data and convert it to an audio segment chunk = await ws.receive_bytes() if chunk == END_OF_STREAM: await ws.send_bytes(END_OF_STREAM) break audio_segment, text = await self.handle_audio_chunk( chunk, audio_segment ) if text: await ws.send_text(text) except Exception as e: if not isinstance(e, WebSocketDisconnect): print(f"Error handling websocket: {type(e)}: {e}") try: await ws.close(code=1011, reason="Internal server error") except Exception as e: print(f"Error closing websocket: {type(e)}: {e}") return web_app @modal.method() async def run_with_queue(self, q: modal.Queue): from pydub import AudioSegment # initialize an empty audio segment audio_segment = AudioSegment.empty() try: while True: # receive a chunk of audio data and convert it to an audio segment chunk = await q.get.aio(partition="audio") if chunk == END_OF_STREAM: await q.put.aio(END_OF_STREAM, partition="transcription") break audio_segment, text = await self.handle_audio_chunk( chunk, audio_segment ) if text: await q.put.aio(text, partition="transcription") except Exception as e: print(f"Error handling queue: {type(e)}: {e}") return async def handle_audio_chunk( self, chunk: bytes, audio_segment, silence_thresh=-45, # dB min_silence_len=1000, # ms ): from pydub import AudioSegment, silence new_audio_segment = AudioSegment( data=chunk, channels=1, sample_width=2, frame_rate=TARGET_SAMPLE_RATE, ) # append the new audio segment to the existing audio segment audio_segment += new_audio_segment # detect windows of silence silent_windows = silence.detect_silence( audio_segment, min_silence_len=min_silence_len, silence_thresh=silence_thresh, ) # if there are no silent windows, continue if len(silent_windows) == 0: return audio_segment, None # get the last silent window because # we want to transcribe until the final pause last_window = silent_windows[-1] # if the entire audio segment is silent, reset the audio segment if last_window[0] == 0 and last_window[1] == len(audio_segment): audio_segment = AudioSegment.empty() return audio_segment, None # get the segment to transcribe: beginning until last pause segment_to_transcribe = audio_segment[: last_window[1]] # remove the segment to transcribe from the audio segment audio_segment = audio_segment[last_window[1] :] try: text = self.transcribe(segment_to_transcribe.raw_data) return audio_segment, text except Exception as e: print("âŒ Transcription error:", e) raise e
```

## Implementing streaming audio transcription on Modal

```
END_OF_STREAM = ( b"END_OF_STREAM_8f13d09" # byte sequence indicating a stream is finished ) @app.cls(volumes={"/cache": model_cache}, gpu="a10g", image=image) @modal.concurrent(max_inputs=14, target_inputs=10) class Parakeet: @modal.enter() def load(self): import logging import nemo.collections.asr as nemo_asr # silence chatty logs from nemo logging.getLogger("nemo_logger").setLevel(logging.CRITICAL) self.model = nemo_asr.models.ASRModel.from_pretrained( model_name="nvidia/parakeet-tdt-0.6b-v2" ) def transcribe(self, audio_bytes: bytes) -> str: import numpy as np audio_data = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) with NoStdStreams(): # hide output, see https://github.com/NVIDIA/NeMo/discussions/3281#discussioncomment-2251217 output = self.model.transcribe([audio_data]) return output[0].text @modal.asgi_app() def web(self): from fastapi import FastAPI, Response, WebSocket from fastapi.responses import HTMLResponse from fastapi.staticfiles import StaticFiles web_app = FastAPI() web_app.mount("/static", StaticFiles(directory="/frontend")) @web_app.get("/status") async def status(): return Response(status_code=200) # serve frontend @web_app.get("/") async def index(): return HTMLResponse(content=open("/frontend/index.html").read()) @web_app.websocket("/ws") async def run_with_websocket(ws: WebSocket): from fastapi import WebSocketDisconnect from pydub import AudioSegment await ws.accept() # initialize an empty audio segment audio_segment = AudioSegment.empty() try: while True: # receive a chunk of audio data and convert it to an audio segment chunk = await ws.receive_bytes() if chunk == END_OF_STREAM: await ws.send_bytes(END_OF_STREAM) break audio_segment, text = await self.handle_audio_chunk( chunk, audio_segment ) if text: await ws.send_text(text) except Exception as e: if not isinstance(e, WebSocketDisconnect): print(f"Error handling websocket: {type(e)}: {e}") try: await ws.close(code=1011, reason="Internal server error") except Exception as e: print(f"Error closing websocket: {type(e)}: {e}") return web_app @modal.method() async def run_with_queue(self, q: modal.Queue): from pydub import AudioSegment # initialize an empty audio segment audio_segment = AudioSegment.empty() try: while True: # receive a chunk of audio data and convert it to an audio segment chunk = await q.get.aio(partition="audio") if chunk == END_OF_STREAM: await q.put.aio(END_OF_STREAM, partition="transcription") break audio_segment, text = await self.handle_audio_chunk( chunk, audio_segment ) if text: await q.put.aio(text, partition="transcription") except Exception as e: print(f"Error handling queue: {type(e)}: {e}") return async def handle_audio_chunk( self, chunk: bytes, audio_segment, silence_thresh=-45, # dB min_silence_len=1000, # ms ): from pydub import AudioSegment, silence new_audio_segment = AudioSegment( data=chunk, channels=1, sample_width=2, frame_rate=TARGET_SAMPLE_RATE, ) # append the new audio segment to the existing audio segment audio_segment += new_audio_segment # detect windows of silence silent_windows = silence.detect_silence( audio_segment, min_silence_len=min_silence_len, silence_thresh=silence_thresh, ) # if there are no silent windows, continue if len(silent_windows) == 0: return audio_segment, None # get the last silent window because # we want to transcribe until the final pause last_window = silent_windows[-1] # if the entire audio segment is silent, reset the audio segment if last_window[0] == 0 and last_window[1] == len(audio_segment): audio_segment = AudioSegment.empty() return audio_segment, None # get the segment to transcribe: beginning until last pause segment_to_transcribe = audio_segment[: last_window[1]] # remove the segment to transcribe from the audio segment audio_segment = audio_segment[last_window[1] :] try: text = self.transcribe(segment_to_transcribe.raw_data) return audio_segment, text except Exception as e: print("âŒ Transcription error:", e) raise e
```

## Running transcription from a local Python client

Next, letâ€™s test the model with a [local_entrypoint](https://modal.com/docs/reference/modal.App) that streams audio data to the server and prints out the transcriptions to our terminal as they arrive.

```
local_entrypoint
```

Instead of using the WebSocket endpoint like the browser frontend, weâ€™ll use a [modal.Queue](https://modal.com/docs/reference/modal.Queue) to pass audio data and transcriptions between our local machine and the GPU container.

```
modal.Queue
```


```
AUDIO_URL = "https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/mono_44100/156550__acclivity__a-dream-within-a-dream.wav" TARGET_SAMPLE_RATE = 16_000 CHUNK_SIZE = 16_000 # send one second of audio at a time @app.local_entrypoint() async def main(audio_url: str = AUDIO_URL): from urllib.request import urlopen print(f"ðŸŒ Downloading audio file from {audio_url}") audio_bytes = urlopen(audio_url).read() print(f"ðŸŽ§ Downloaded {len(audio_bytes)} bytes") audio_data = preprocess_audio(audio_bytes) print("ðŸŽ¤ Starting Transcription") with modal.Queue.ephemeral() as q: Parakeet().run_with_queue.spawn(q) send = asyncio.create_task(send_audio(q, audio_data)) recv = asyncio.create_task(receive_text(q)) await asyncio.gather(send, recv) print("âœ… Transcription complete!")
```


```
AUDIO_URL = "https://github.com/voxserv/audio_quality_testing_samples/raw/refs/heads/master/mono_44100/156550__acclivity__a-dream-within-a-dream.wav" TARGET_SAMPLE_RATE = 16_000 CHUNK_SIZE = 16_000 # send one second of audio at a time @app.local_entrypoint() async def main(audio_url: str = AUDIO_URL): from urllib.request import urlopen print(f"ðŸŒ Downloading audio file from {audio_url}") audio_bytes = urlopen(audio_url).read() print(f"ðŸŽ§ Downloaded {len(audio_bytes)} bytes") audio_data = preprocess_audio(audio_bytes) print("ðŸŽ¤ Starting Transcription") with modal.Queue.ephemeral() as q: Parakeet().run_with_queue.spawn(q) send = asyncio.create_task(send_audio(q, audio_data)) recv = asyncio.create_task(receive_text(q)) await asyncio.gather(send, recv) print("âœ… Transcription complete!")
```

Below are the two functions that coordinate streaming audio and receiving transcriptions.
send_audio transmits chunks of audio data with a slight delay, as though it was being streamed from a live source, like a microphone. receive_text waits for transcribed text to arrive and prints it.

```
send_audio
```


```
receive_text
```


```
async def send_audio(q, audio_bytes): for chunk in chunk_audio(audio_bytes, CHUNK_SIZE): await q.put.aio(chunk, partition="audio") await asyncio.sleep(CHUNK_SIZE / TARGET_SAMPLE_RATE / 8) await q.put.aio(END_OF_STREAM, partition="audio") async def receive_text(q): while True: message = await q.get.aio(partition="transcription") if message == END_OF_STREAM: break print(message)
```


```
async def send_audio(q, audio_bytes): for chunk in chunk_audio(audio_bytes, CHUNK_SIZE): await q.put.aio(chunk, partition="audio") await asyncio.sleep(CHUNK_SIZE / TARGET_SAMPLE_RATE / 8) await q.put.aio(END_OF_STREAM, partition="audio") async def receive_text(q): while True: message = await q.get.aio(partition="transcription") if message == END_OF_STREAM: break print(message)
```

## Addenda

The remainder of the code in this example is boilerplate, mostly for handling Parakeetâ€™s input format.

```
def preprocess_audio(audio_bytes: bytes) -> bytes: import array import io import wave with wave.open(io.BytesIO(audio_bytes), "rb") as wav_in: n_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() frame_rate = wav_in.getframerate() n_frames = wav_in.getnframes() frames = wav_in.readframes(n_frames) # Convert frames to array based on sample width if sample_width == 1: audio_data = array.array("B", frames) # unsigned char elif sample_width == 2: audio_data = array.array("h", frames) # signed short elif sample_width == 4: audio_data = array.array("i", frames) # signed int else: raise ValueError(f"Unsupported sample width: {sample_width}") # Downmix to mono if needed if n_channels > 1: mono_data = array.array(audio_data.typecode) for i in range(0, len(audio_data), n_channels): chunk = audio_data[i : i + n_channels] mono_data.append(sum(chunk) // n_channels) audio_data = mono_data # Resample to 16kHz if needed if frame_rate != TARGET_SAMPLE_RATE: ratio = TARGET_SAMPLE_RATE / frame_rate new_length = int(len(audio_data) * ratio) resampled_data = array.array(audio_data.typecode) for i in range(new_length): # Linear interpolation pos = i / ratio pos_int = int(pos) pos_frac = pos - pos_int if pos_int >= len(audio_data) - 1: sample = audio_data[-1] else: sample1 = audio_data[pos_int] sample2 = audio_data[pos_int + 1] sample = int(sample1 + (sample2 - sample1) * pos_frac) resampled_data.append(sample) audio_data = resampled_data return audio_data.tobytes() def chunk_audio(data: bytes, chunk_size: int): for i in range(0, len(data), chunk_size): yield data[i : i + chunk_size] class NoStdStreams(object): def __init__(self): self.devnull = open(os.devnull, "w") def __enter__(self): self._stdout, self._stderr = sys.stdout, sys.stderr self._stdout.flush(), self._stderr.flush() sys.stdout, sys.stderr = self.devnull, self.devnull def __exit__(self, exc_type, exc_value, traceback): sys.stdout, sys.stderr = self._stdout, self._stderr self.devnull.close()
```


```
def preprocess_audio(audio_bytes: bytes) -> bytes: import array import io import wave with wave.open(io.BytesIO(audio_bytes), "rb") as wav_in: n_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() frame_rate = wav_in.getframerate() n_frames = wav_in.getnframes() frames = wav_in.readframes(n_frames) # Convert frames to array based on sample width if sample_width == 1: audio_data = array.array("B", frames) # unsigned char elif sample_width == 2: audio_data = array.array("h", frames) # signed short elif sample_width == 4: audio_data = array.array("i", frames) # signed int else: raise ValueError(f"Unsupported sample width: {sample_width}") # Downmix to mono if needed if n_channels > 1: mono_data = array.array(audio_data.typecode) for i in range(0, len(audio_data), n_channels): chunk = audio_data[i : i + n_channels] mono_data.append(sum(chunk) // n_channels) audio_data = mono_data # Resample to 16kHz if needed if frame_rate != TARGET_SAMPLE_RATE: ratio = TARGET_SAMPLE_RATE / frame_rate new_length = int(len(audio_data) * ratio) resampled_data = array.array(audio_data.typecode) for i in range(new_length): # Linear interpolation pos = i / ratio pos_int = int(pos) pos_frac = pos - pos_int if pos_int >= len(audio_data) - 1: sample = audio_data[-1] else: sample1 = audio_data[pos_int] sample2 = audio_data[pos_int + 1] sample = int(sample1 + (sample2 - sample1) * pos_frac) resampled_data.append(sample) audio_data = resampled_data return audio_data.tobytes() def chunk_audio(data: bytes, chunk_size: int): for i in range(0, len(data), chunk_size): yield data[i : i + chunk_size] class NoStdStreams(object): def __init__(self): self.devnull = open(os.devnull, "w") def __enter__(self): self._stdout, self._stderr = sys.stdout, sys.stderr self._stdout.flush(), self._stderr.flush() sys.stdout, sys.stderr = self.devnull, self.devnull def __exit__(self, exc_type, exc_value, traceback): sys.stdout, sys.stderr = self._stdout, self._stderr self.devnull.close()
```

Streaming audio transcription using Parakeet
Setup
Volume for caching model weights
Configuring dependencies
Implementing streaming audio transcription on Modal
Running transcription from a local Python client
Addenda

## Try this on Modal!

You can run this example on Modal in 60 seconds.
[Create account to run](https://modal.com/signup)
After creating a free account, install the Modal Python package, and create an API token.

```
pip install modal
```


```
pip install modal
```


```
modal setup
```


```
modal setup
```

Clone the [modal-examples](https://github.com/modal-labs/modal-examples) repository and run:

```
git clone https://github.com/modal-labs/modal-examples
```


```
git clone https://github.com/modal-labs/modal-examples
```


```
cd modal-examples
```


```
cd modal-examples
```


```
modal run 06_gpu_and_ml/speech-to-text/streaming_parakeet.py
```


```
modal run 06_gpu_and_ml/speech-to-text/streaming_parakeet.py
```

[About](https://modal.com/company)
[Status](https://status.modal.com/)
[Changelog](https://modal.com/docs/reference/changelog)
[Documentation](https://modal.com/docs/guide)
[Slack Community](https://modal.com/slack)
[Pricing](https://modal.com/pricing)
[Examples](https://modal.com/docs/examples)